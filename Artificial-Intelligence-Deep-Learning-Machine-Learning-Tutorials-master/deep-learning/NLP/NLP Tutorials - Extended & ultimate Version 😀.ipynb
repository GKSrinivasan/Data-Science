{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC \n",
    "\n",
    "0. **Text Preprocesing**\n",
    "1. **Feature Engineering** (Parsing, Entity Extraction, Statistical Features such as Tf-IDF, Word Embeddings - gentle intro, later more), N-grams and more.\n",
    "2. **Stemming**\n",
    "3. **Word Embeddings**\n",
    "4. **Part of speech Tagging**\n",
    "5. **Named entity Disabiguation**\n",
    "6. **Named entity Recognition**\n",
    "7. **Sentiment Analysis**\n",
    "8. **Semantic Text Similarity**\n",
    "9. **Language Identification**\n",
    "10. **Text Summarization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Preprocessing\n",
    "\n",
    "This involves three simple steps, namely:\n",
    "- Denoising\n",
    "- Normalizing Lexicon\n",
    "- Standardizing Objects\n",
    "\n",
    "**DeNoising**\n",
    "\n",
    "Here we get rid of language stopwords -- as you've seen in other tutorials that sometimes they're helpful(multi-lingual stuff), other times not.\n",
    "\n",
    "Let's Denoise some stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_stuff = ['is', 'the', 'a', '...']\n",
    "def denoise(input_text):\n",
    "    words = input_text.split()\n",
    "    noise_free_words = [word for word in words if word not in noise_stuff]\n",
    "    noise_free_text = \" \".join(noise_free_words)\n",
    "    return noise_free_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat real clown and it drives me'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denoise('the cat is a real clown and it drives me ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalizing Lexicon**\n",
    "\n",
    "Here we touched also briefly about words such beauty, beautiful, beautify, which are all variations of beauty or more specifically \"beaut\" - which is what Stemming does\n",
    "\n",
    "- **Stemming** : This is a rudimentary rule-based process that strips the suffixes\n",
    "- **Lemmetization**: does something else. It goes step-by-step in an organized manner to to obtain root of the word.\n",
    "\n",
    "Let's take a look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beautify\n",
      "beautifi\n"
     ]
    }
   ],
   "source": [
    "# We import them both\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Assign\n",
    "lemm = WordNetLemmatizer()\n",
    "stemm = PorterStemmer()\n",
    "\n",
    "word = \"beautifying\"\n",
    "print(lemm.lemmatize(word, 'v'))\n",
    "print(stemm.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardizing Object**\n",
    "\n",
    "Text data might contain word or phrases which do not exist in dictionaries and may not find their way to search engines etc. Think of acronyms, hashtags, some other new trending stuff on the internet. [Note: urban dictionary does capture a lot of stuff lately though]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {'rt': 'Retweet', 'dm':'Direct Message', 'lol':\"Laughing out loud\", 'wtf': 'What the funk'}\n",
    "\n",
    "def lookup_words(input_text):\n",
    "    words = input_text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word)\n",
    "        new_text = \" \".join(new_words)\n",
    "        return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What the funk'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_words(\"wtf dude I sent you a dm but you never replied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Here we will discuss and play around a bit with the following:\n",
    "\n",
    "- Parsing\n",
    "- Entity extraction\n",
    "- Term Frequency / Inverse Document Frequency (TF-IDF)\n",
    "- Word Embedding (gentle intro, more later)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are N-grams \n",
    "\n",
    "Coming soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is'], ['is', 'a'], ['a', 'sample'], ['sample', 'text']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "\n",
    "generate_ngrams('this is a sample text', 2)\n",
    "# [['this', 'is'], ['is', 'a'], ['a', 'sample'], , ['sample', 'text']] \n",
    "# Play around with parameters N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Necessary functions\n",
    "\n",
    "- `function index_to_hot()`\n",
    "\n",
    "```python\n",
    "Converts index value to one hot vector.\n",
    "    e.g. [2, 5] (with 10 classes) becomes:\n",
    "        [\n",
    "            [0 0 1 0 0 0 0 0 0 0]\n",
    "            [0 0 0 0 1 0 0 0 0 0]\n",
    "        ]\n",
    "```\n",
    "- `function squash()`\n",
    "```python\n",
    "    Non-linear 'squashing' to ensure short vectors get shrunk\n",
    "    to almost zero length and long vectors get shrunk to a\n",
    "    length slightly below 1.\n",
    "```\n",
    "- `function softmax()`\n",
    "```python\n",
    "    Apply softmax to specific dimensions. Not released on PyTorch stable yet\n",
    "    as of November 6th 2017\n",
    "    https://github.com/pytorch/pytorch/issues/3235\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_to_one_hot(index_tensor, num_classes=10):\n",
    "    index_tensor = index_tensor.long()\n",
    "    return torch.eye(num_classes).index_select(dim=0, index=index_tensor)\n",
    "\n",
    "\n",
    "def squash_vector(tensor, dim=-1):\n",
    "    squared_norm = (tensor**2).sum(dim=dim, keepdim=True)\n",
    "    scale = squared_norm / (1 + squared_norm)\n",
    "    return scale * tensor / torch.sqrt(squared_norm)\n",
    "\n",
    "\n",
    "def softmax(input, dim=1):\n",
    "    transposed_input = input.transpose(dim, len(input.size()) - 1)\n",
    "    softmaxed_output = F.softmax(\n",
    "        transposed_input.contiguous().view(-1, transposed_input.size(-1)))\n",
    "    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input.size()) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Class CapsuleLayer\n",
    "\n",
    "- Init\n",
    "- Forward with routing defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_capsules, num_routes, in_channels, out_channels,\n",
    "                 kernel_size=None, stride=None, num_iterations=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_routes = num_routes\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        if num_routes != -1:\n",
    "            self.route_weights = nn.Parameter(\n",
    "                torch.randn(num_capsules, num_routes,\n",
    "                            in_channels, out_channels)\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.capsules = nn.ModuleList(\n",
    "                [nn.Conv2d(in_channels,\n",
    "                           out_channels,\n",
    "                           kernel_size=kernel_size,\n",
    "                           stride=stride,\n",
    "                           padding=0)\n",
    "                 for _ in range(num_capsules)\n",
    "                 ]\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If routing is defined\n",
    "        if self.num_routes != -1:\n",
    "            priors = x[None, :, :, None, :] @ self.route_weights[:, None, :, :, :]\n",
    "\n",
    "            logits = Variable(torch.zeros(priors.size())).cuda()\n",
    "\n",
    "            # Routing algorithm\n",
    "            for i in range(self.num_iterations):\n",
    "                probs = softmax(logits, dim=2)\n",
    "                outputs = squash_vector(\n",
    "                    probs * priors).sum(dim=2, keepdim=True)\n",
    "\n",
    "                if i != self.num_iterations - 1:\n",
    "                    delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "                    logits = logits + delta_logits\n",
    "\n",
    "        else:\n",
    "            outputs = [capsule(x).view(x.size(0), -1, 1)\n",
    "                       for capsule in self.capsules]\n",
    "            outputs = torch.cat(outputs, dim=-1)\n",
    "            outputs = squash_vector(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Class MarginLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MarginLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Reconstruction as regularization\n",
    "        self.reconstruction_loss = nn.MSELoss(size_average=False)\n",
    "\n",
    "    def forward(self, images, labels, classes, reconstructions):\n",
    "        left = F.relu(0.9 - classes, inplace=True) ** 2\n",
    "        right = F.relu(classes - 0.1, inplace=True) ** 2\n",
    "        margin_loss = labels * left + 0.5 * (1. - labels) * right\n",
    "        margin_loss = margin_loss.sum()\n",
    "        reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n",
    "\n",
    "        return (margin_loss + 0.0005 * reconstruction_loss) / images.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Capsule Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=256, kernel_size=9, stride=1)\n",
    "        self.primary_capsules = CapsuleLayer(\n",
    "            8, -1, 256, 32, kernel_size=9, stride=2)\n",
    "\n",
    "        # 10 is the number of classes\n",
    "        self.digit_capsules = CapsuleLayer(10, 32 * 6 * 6, 8, 16)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16 * 10, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = F.relu(self.conv1(x), inplace=True)\n",
    "        x = self.primary_capsules(x)\n",
    "        x = self.digit_capsules(x).squeeze().transpose(0, 1)\n",
    "\n",
    "        classes = (x ** 2).sum(dim=-1) ** 0.5\n",
    "        classes = F.softmax(classes)\n",
    "\n",
    "        if y is None:\n",
    "            # In all batches, get the most active capsule\n",
    "            _, max_length_indices = classes.max(dim=1)\n",
    "            y = Variable(torch.eye(10)).cuda().index_select(\n",
    "                dim=0, index=max_length_indices.data)\n",
    "\n",
    "        reconstructions = self.decoder((x * y[:, :, None]).view(x.size(0), -1))\n",
    "        return classes, reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Hyperparameters, Download, Process, Load & Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Globals\n",
    "CUDA = True\n",
    "EPOCH = 10\n",
    "\n",
    "# Model\n",
    "model = CapsuleNet()\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "margin_loss = MarginLoss()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MNIST(root='/tmp', download=True, train=True,\n",
    "            transform=transforms.ToTensor()),\n",
    "    batch_size=8, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    MNIST(root='/tmp', download=True, train=False,\n",
    "            transform=transforms.ToTensor()),\n",
    "    batch_size=8, shuffle=True)\n",
    "\n",
    "for e in range(10):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for idx, (img, target) in enumerate(tqdm(train_loader)):\n",
    "        img = Variable(img)\n",
    "        target = Variable(index_to_one_hot(target))\n",
    "\n",
    "        if CUDA:\n",
    "            img = img.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        classes, reconstructions = model(img, target)\n",
    "\n",
    "        loss = margin_loss(img, target, classes, reconstructions)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Iter: {}, Loss: {:.4f}'.format(idx, loss.data[0]))\n",
    "\n",
    "        # Testing\n",
    "        correct = 0\n",
    "        test_loss = 0\n",
    "\n",
    "        model.eval()\n",
    "    for idx, (img, target) in enumerate(test_loader):\n",
    "        img = Variable(img)\n",
    "        target = Variable(index_to_one_hot(target))\n",
    "\n",
    "        if CUDA:\n",
    "            img = img.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        classes, recontructions = model(img, target)\n",
    "\n",
    "        test_loss += margin_loss(img, target, classes, reconstructions)\n",
    "\n",
    "        # Get index of the max log-probability\n",
    "        pred = classes.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test Set: Avg Loss: {:.4f}/{}, Accuracy: {}/{}'.format(\n",
    "        test_loss.data[0], correct, len(test_loader.dataset, 100. * correct / len(test_loader.dataset))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

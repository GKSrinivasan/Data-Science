{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
      "             evaluate_every=-1, learning_decay=0.7,\n",
      "             learning_method='batch', learning_offset=10.0,\n",
      "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
      "             n_components=10, n_jobs=1, n_topics=40, perp_tol=0.1,\n",
      "             random_state=None, topic_word_prior=None,\n",
      "             total_samples=1000000.0, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "clf = joblib.load('ldaModel.pkl')\n",
    "print(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['order'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ec567775fbe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"order\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mDocument\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0mdistribution\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \"\"\"\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0mdoc_topic_distr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unnormalized_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0mdoc_topic_distr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mdoc_topic_distr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_topic_distr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36m_unnormalized_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;31m# make sure feature size is the same in fitted model and in X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_non_neg_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LatentDirichletAllocation.transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py\u001b[0m in \u001b[0;36m_check_non_neg_array\u001b[0;34m(self, X, whom)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \"\"\"\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['order'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "clf.transform([\"order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict([\"hi my mouse is having issue in connectivity\"])\n",
    "print(jsonify({'prediction': list(prediction)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'outfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d6cf4010fb8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# load parameters from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'outfile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_dirichlet_component_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_topic_prior_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'outfile'"
     ]
    }
   ],
   "source": [
    "# derived from http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html\n",
    "# explanations are located there : https://www.linkedin.com/pulse/dissociating-training-predicting-latent-dirichlet-lucien-tardres\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "# create a blank model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# load parameters from file\n",
    "with open ('outfile', 'rb') as fd:\n",
    "    (features,lda.components_,lda.exp_dirichlet_component_,lda.doc_topic_prior_) = pickle.load(fd)\n",
    "joblib.load('ldaModel.pkl')\n",
    "# the dataset to predict on (first two samples were also in the training set so one can compare)\n",
    "data_samples = [\"I like to eat broccoli and bananas.\",\n",
    "                \"I ate a banana and spinach smoothie for breakfast.\",\n",
    "                \"kittens and dogs are boring\"\n",
    "               ]\n",
    "# Vectorize the training set using the model features as vocabulary\n",
    "tf_vectorizer = CountVectorizer(vocabulary=features)\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "# transform method returns a matrix with one line per document, columns being topics weight\n",
    "predict = lda.transform(tf)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people time right did good said say make way government\n",
      "Topic 1:\n",
      "window problem using server application screen display motif manager running\n",
      "Topic 2:\n",
      "god jesus bible christ faith believe christian christians sin church\n",
      "Topic 3:\n",
      "game team year games season players play hockey win league\n",
      "Topic 4:\n",
      "new 00 sale 10 price offer shipping condition 20 15\n",
      "Topic 5:\n",
      "thanks mail advance hi looking info help information address appreciated\n",
      "Topic 6:\n",
      "windows file files dos program version ftp ms directory running\n",
      "Topic 7:\n",
      "edu soon cs university ftp internet article email pub david\n",
      "Topic 8:\n",
      "key chip clipper encryption keys escrow government public algorithm nsa\n",
      "Topic 9:\n",
      "drive scsi drives hard disk ide floppy controller cd mac\n",
      "Topic 10:\n",
      "just ll thought tell oh little fine work wanted mean\n",
      "Topic 11:\n",
      "does know anybody mean work say doesn help exist program\n",
      "Topic 12:\n",
      "card video monitor cards drivers bus vga driver color memory\n",
      "Topic 13:\n",
      "like sounds looks look bike sound lot things really thing\n",
      "Topic 14:\n",
      "don know want let need doesn little sure sorry things\n",
      "Topic 15:\n",
      "car cars engine speed good bike driver road insurance fast\n",
      "Topic 16:\n",
      "ve got seen heard tried good recently times try couple\n",
      "Topic 17:\n",
      "use used using work available want software need image data\n",
      "Topic 18:\n",
      "think don lot try makes really pretty wasn bit david\n",
      "Topic 19:\n",
      "com list dave internet article sun hp email ibm phone\n",
      "Topic 0:\n",
      "people gun state control right guns crime states law police\n",
      "Topic 1:\n",
      "time question book years did like don space answer just\n",
      "Topic 2:\n",
      "mr line rules science stephanopoulos title current define int yes\n",
      "Topic 3:\n",
      "key chip keys clipper encryption number des algorithm use bit\n",
      "Topic 4:\n",
      "edu com cs vs w7 cx mail uk 17 send\n",
      "Topic 5:\n",
      "use does window problem way used point different case value\n",
      "Topic 6:\n",
      "windows thanks know help db does dos problem like using\n",
      "Topic 7:\n",
      "bike water effect road design media dod paper like turn\n",
      "Topic 8:\n",
      "don just like think know people good ve going say\n",
      "Topic 9:\n",
      "car new price good power used air sale offer ground\n",
      "Topic 10:\n",
      "file available program edu ftp information files use image version\n",
      "Topic 11:\n",
      "ax max b8f g9v a86 145 pl 1d9 0t 34u\n",
      "Topic 12:\n",
      "government law privacy security legal encryption court fbi technology information\n",
      "Topic 13:\n",
      "card bit memory output video color data mode monitor 16\n",
      "Topic 14:\n",
      "drive scsi disk mac hard apple drives controller software port\n",
      "Topic 15:\n",
      "god jesus people believe christian bible say does life church\n",
      "Topic 16:\n",
      "year game team games season play hockey players league player\n",
      "Topic 17:\n",
      "10 00 15 25 20 11 12 14 16 13\n",
      "Topic 18:\n",
      "armenian israel armenians war people jews turkish israeli said women\n",
      "Topic 19:\n",
      "president people new said health year university school day work\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived from http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html\n",
    "# explanations are located there : https://www.linkedin.com/pulse/dissociating-training-predicting-latent-dirichlet-lucien-tardres\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "n_features = 50\n",
    "n_topics = 2\n",
    "\n",
    "# Training dataset\n",
    "data_samples = [\"I like to eat broccoli and bananas.\",\n",
    "                \"I ate a banana and spinach smoothie for breakfast.\",\n",
    "                \"Chinchillas and kittens are cute.\",\n",
    "                \"My sister adopted a kitten yesterday.\",\n",
    "                \"Look at this cute hamster munching on a piece of broccoli.\"\n",
    "               ]\n",
    "# extract fetures and vectorize dataset\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=1,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "#save features\n",
    "dic = tf_vectorizer.get_feature_names()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "# train LDA\n",
    "p1 = lda.fit(tf)\n",
    "\n",
    "# Save all data necessary for later prediction\n",
    "model = (dic,lda.components_,lda.exp_dirichlet_component_,lda.doc_topic_prior_)\n",
    "\n",
    "with open('outfile', 'wb') as fp:\n",
    "    pickle.dump(model, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30037154 0.69962846]\n",
      " [0.37713139 0.62286861]\n",
      " [0.31943191 0.68056809]]\n"
     ]
    }
   ],
   "source": [
    "# derived from http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html\n",
    "# explanations are located there : https://www.linkedin.com/pulse/dissociating-training-predicting-latent-dirichlet-lucien-tardres\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle\n",
    "\n",
    "# create a blank model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# load parameters from file\n",
    "with open ('outfile', 'rb') as fd:\n",
    "    (features,lda.components_,lda.exp_dirichlet_component_,lda.doc_topic_prior_) = pickle.load(fd)\n",
    "\n",
    "# the dataset to predict on (first two samples were also in the training set so one can compare)\n",
    "data_samples = [\"who I like to eat  and bananas. what the fuck\",\n",
    "                \"I ate a banana and spinach smoothie for breakfast.\",\n",
    "                \"I like to eat broccoli and bananas.\"\n",
    "               ]\n",
    "# Vectorize the training set using the model features as vocabulary\n",
    "tf_vectorizer = TfidfVectorizer(vocabulary=features)\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "# transform method returns a matrix with one line per document, columns being topics weight\n",
    "predict = lda.transform(tf)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pyLDAvis/_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el81914112006599808847606404\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el81914112006599808847606404_data = {\"mdsDat\": {\"Freq\": [66.44388754583993, 33.55611245416007], \"cluster\": [1, 1], \"topics\": [1, 2], \"x\": [0.0014334244552320255, -0.0014334244552320255], \"y\": [0.0, 0.0]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"Freq\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20425744188486986, 0.21890905349865267, 0.20484550553049413, 0.2260800762952313, 0.190030880373362, 0.2094319735586541, 0.1969280897727567, 0.19678790327610468, 0.2016056439465896, 0.21185376408513384, 0.20426275445833336, 0.22014707594421723, 0.1690620183622018, 0.1998815154857993, 0.18890493178893503, 0.1982306248780272, 0.1861872154390206, 0.17431545383733052, 0.1723775626507897, 0.1808609629168711, 0.11763891961696048, 0.11161436227010912, 0.10782209640182437, 0.11040954012025256, 0.11050231603119459, 0.1037930059958847, 0.10732444499599253, 0.08822734456601382, 0.11226799420198262, 0.10374211582916659, 0.10613767134189495, 0.10097432191340716, 0.09716707326386105, 0.09355488216704269, 0.09866783950260255, 0.08625658304916155, 0.09679755715368715, 0.08295634619740987, 0.08690061445448932, 0.07461602259417069], \"Term\": [\"munching\", \"sister\", \"chinchillas\", \"banana\", \"kitten\", \"look\", \"cute\", \"adopted\", \"broccoli\", \"smoothie\", \"hamster\", \"piece\", \"eat\", \"kittens\", \"breakfast\", \"bananas\", \"ate\", \"yesterday\", \"spinach\", \"like\", \"like\", \"spinach\", \"yesterday\", \"ate\", \"bananas\", \"kittens\", \"breakfast\", \"eat\", \"hamster\", \"smoothie\", \"broccoli\", \"adopted\", \"piece\", \"cute\", \"look\", \"kitten\", \"banana\", \"chinchillas\", \"sister\", \"munching\", \"munching\", \"sister\", \"chinchillas\", \"banana\", \"kitten\", \"look\", \"cute\", \"piece\", \"adopted\", \"broccoli\", \"smoothie\", \"hamster\", \"eat\", \"breakfast\", \"kittens\", \"bananas\", \"ate\", \"yesterday\", \"spinach\", \"like\"], \"Total\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27887346447904054, 0.305809667953142, 0.287801851727904, 0.32287763344891846, 0.2762874634225235, 0.30809981306125667, 0.2904829719397994, 0.2939549765399657, 0.3025799658599968, 0.3179914354270288, 0.30800487028749995, 0.33241507014619986, 0.25728936292821564, 0.30720596048179183, 0.29269793778481973, 0.30873294090922176, 0.2965967555592731, 0.2821375502391549, 0.28399192492089886, 0.2984998825338316, 0.2984998825338316, 0.28399192492089886, 0.2821375502391549, 0.2965967555592731, 0.30873294090922176, 0.29269793778481973, 0.30720596048179183, 0.25728936292821564, 0.33241507014619986, 0.30800487028749995, 0.3179914354270288, 0.3025799658599968, 0.2939549765399657, 0.2904829719397994, 0.30809981306125667, 0.2762874634225235, 0.32287763344891846, 0.287801851727904, 0.305809667953142, 0.27887346447904054], \"loglift\": [20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.0974, 0.0745, 0.0688, 0.0524, 0.0346, 0.0228, 0.0201, 0.0075, 0.0028, 0.0027, -0.0019, -0.0033, -0.0111, -0.021, -0.0291, -0.0342, -0.0568, -0.0727, -0.0904, -0.0922, 0.1608, 0.1581, 0.13, 0.1038, 0.0645, 0.0552, 0.0403, 0.0217, 0.0065, 0.0037, -0.0053, -0.0055, -0.015, -0.041, -0.0467, -0.0722, -0.1127, -0.152, -0.1662, -0.2265], \"logprob\": [20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.9633, -2.8941, -2.9605, -2.8618, -3.0355, -2.9383, -2.9999, -3.0006, -2.9764, -2.9268, -2.9633, -2.8884, -3.1525, -2.985, -3.0415, -2.9933, -3.056, -3.1219, -3.133, -3.085, -2.832, -2.8845, -2.9191, -2.8954, -2.8946, -2.9572, -2.9237, -3.1197, -2.8787, -2.9577, -2.9349, -2.9847, -3.0232, -3.061, -3.0078, -3.1423, -3.027, -3.1813, -3.1348, -3.2872]}, \"token.table\": {\"Topic\": [], \"Freq\": [], \"Term\": []}, \"R\": 20, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el81914112006599808847606404\", ldavis_el81914112006599808847606404_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el81914112006599808847606404\", ldavis_el81914112006599808847606404_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el81914112006599808847606404\", ldavis_el81914112006599808847606404_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics         x    y\n",
       "topic                                           \n",
       "1      66.443888        1       1  0.001433  0.0\n",
       "0      33.556112        1       2 -0.001433  0.0, topic_info=     Category      Freq         Term     Total  loglift  logprob\n",
       "term                                                            \n",
       "14    Default  0.000000     munching  0.000000  20.0000  20.0000\n",
       "16    Default  0.000000       sister  0.000000  19.0000  19.0000\n",
       "6     Default  0.000000  chinchillas  0.000000  18.0000  18.0000\n",
       "2     Default  0.000000       banana  0.000000  17.0000  17.0000\n",
       "10    Default  0.000000       kitten  0.000000  16.0000  16.0000\n",
       "13    Default  0.000000         look  0.000000  15.0000  15.0000\n",
       "7     Default  0.000000         cute  0.000000  14.0000  14.0000\n",
       "0     Default  0.000000      adopted  0.000000  13.0000  13.0000\n",
       "5     Default  0.000000     broccoli  0.000000  12.0000  12.0000\n",
       "17    Default  0.000000     smoothie  0.000000  11.0000  11.0000\n",
       "9     Default  0.000000      hamster  0.000000  10.0000  10.0000\n",
       "15    Default  0.000000        piece  0.000000   9.0000   9.0000\n",
       "8     Default  0.000000          eat  0.000000   8.0000   8.0000\n",
       "11    Default  0.000000      kittens  0.000000   7.0000   7.0000\n",
       "4     Default  0.000000    breakfast  0.000000   6.0000   6.0000\n",
       "3     Default  0.000000      bananas  0.000000   5.0000   5.0000\n",
       "1     Default  0.000000          ate  0.000000   4.0000   4.0000\n",
       "19    Default  0.000000    yesterday  0.000000   3.0000   3.0000\n",
       "18    Default  0.000000      spinach  0.000000   2.0000   2.0000\n",
       "12    Default  0.000000         like  0.000000   1.0000   1.0000\n",
       "12     Topic1  0.204257         like  0.278873   0.0974  -2.9633\n",
       "18     Topic1  0.218909      spinach  0.305810   0.0745  -2.8941\n",
       "19     Topic1  0.204846    yesterday  0.287802   0.0688  -2.9605\n",
       "1      Topic1  0.226080          ate  0.322878   0.0524  -2.8618\n",
       "3      Topic1  0.190031      bananas  0.276287   0.0346  -3.0355\n",
       "11     Topic1  0.209432      kittens  0.308100   0.0228  -2.9383\n",
       "4      Topic1  0.196928    breakfast  0.290483   0.0201  -2.9999\n",
       "8      Topic1  0.196788          eat  0.293955   0.0075  -3.0006\n",
       "9      Topic1  0.201606      hamster  0.302580   0.0028  -2.9764\n",
       "17     Topic1  0.211854     smoothie  0.317991   0.0027  -2.9268\n",
       "5      Topic1  0.204263     broccoli  0.308005  -0.0019  -2.9633\n",
       "0      Topic1  0.220147      adopted  0.332415  -0.0033  -2.8884\n",
       "15     Topic1  0.169062        piece  0.257289  -0.0111  -3.1525\n",
       "7      Topic1  0.199882         cute  0.307206  -0.0210  -2.9850\n",
       "13     Topic1  0.188905         look  0.292698  -0.0291  -3.0415\n",
       "10     Topic1  0.198231       kitten  0.308733  -0.0342  -2.9933\n",
       "2      Topic1  0.186187       banana  0.296597  -0.0568  -3.0560\n",
       "6      Topic1  0.174315  chinchillas  0.282138  -0.0727  -3.1219\n",
       "16     Topic1  0.172378       sister  0.283992  -0.0904  -3.1330\n",
       "14     Topic1  0.180861     munching  0.298500  -0.0922  -3.0850\n",
       "14     Topic2  0.117639     munching  0.298500   0.1608  -2.8320\n",
       "16     Topic2  0.111614       sister  0.283992   0.1581  -2.8845\n",
       "6      Topic2  0.107822  chinchillas  0.282138   0.1300  -2.9191\n",
       "2      Topic2  0.110410       banana  0.296597   0.1038  -2.8954\n",
       "10     Topic2  0.110502       kitten  0.308733   0.0645  -2.8946\n",
       "13     Topic2  0.103793         look  0.292698   0.0552  -2.9572\n",
       "7      Topic2  0.107324         cute  0.307206   0.0403  -2.9237\n",
       "15     Topic2  0.088227        piece  0.257289   0.0217  -3.1197\n",
       "0      Topic2  0.112268      adopted  0.332415   0.0065  -2.8787\n",
       "5      Topic2  0.103742     broccoli  0.308005   0.0037  -2.9577\n",
       "17     Topic2  0.106138     smoothie  0.317991  -0.0053  -2.9349\n",
       "9      Topic2  0.100974      hamster  0.302580  -0.0055  -2.9847\n",
       "8      Topic2  0.097167          eat  0.293955  -0.0150  -3.0232\n",
       "4      Topic2  0.093555    breakfast  0.290483  -0.0410  -3.0610\n",
       "11     Topic2  0.098668      kittens  0.308100  -0.0467  -3.0078\n",
       "3      Topic2  0.086257      bananas  0.276287  -0.0722  -3.1423\n",
       "1      Topic2  0.096798          ate  0.322878  -0.1127  -3.0270\n",
       "19     Topic2  0.082956    yesterday  0.287802  -0.1520  -3.1813\n",
       "18     Topic2  0.086901      spinach  0.305810  -0.1662  -3.1348\n",
       "12     Topic2  0.074616         like  0.278873  -0.2265  -3.2872, token_table=Empty DataFrame\n",
       "Columns: [Topic, Freq, Term]\n",
       "Index: [], R=20, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score method\n",
    "The Z-score, or standard score, is a way of describing a data point in terms of its relationship to the mean and standard deviation of a group of points. Taking a Z-score is simply mapping the data onto a distribution whose mean is defined as 0 and whose standard deviation is defined as 1.\n",
    "\n",
    "The goal of taking Z-scores is to remove the effects of the location and scale of the data, allowing different datasets to be compared directly. The intuition behind the Z-score method of outlier detection is that, once we’ve centred and rescaled the data, anything that is too far from zero (the threshold is usually a Z-score of 3 or -3) should be considered an outlier.\n",
    "\n",
    "This function shows how the calculation is made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def outliers_z_score(ys):\n",
    "    threshold = 3\n",
    "\n",
    "    mean_y = np.mean(ys)\n",
    "    stdev_y = np.std(ys)\n",
    "    z_scores = [(y - mean_y) / stdev_y for y in ys]\n",
    "    return np.where(np.abs(z_scores) > threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Z-score method relies on the mean and standard deviation of a group of data to measure central tendency and dispersion. This is troublesome, because the mean and standard deviation are highly affected by outliers – they are not robust. In fact, the skewing that outliers bring is one of the biggest reasons for finding and removing outliers from a dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified Z-score method\n",
    "Another drawback of the Z-score method is that it behaves strangely in small datasets – in fact, the Z-score method will never detect an outlier if the dataset has fewer than 12 items in it. This motivated the development of a modified Z-score method, which does not suffer from the same limitation.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def outliers_modified_z_score(ys):\n",
    "    threshold = 3.5\n",
    "\n",
    "    median_y = np.median(ys)\n",
    "    median_absolute_deviation_y = np.median([np.abs(y - median_y) for y in ys])\n",
    "    modified_z_scores = [0.6745 * (y - median_y) / median_absolute_deviation_y\n",
    "                         for y in ys]\n",
    "    return np.where(np.abs(modified_z_scores) > threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further benefit of the modified Z-score method is that it uses the median and MAD rather than the mean and standard deviation. The median and MAD are robust measures of central tendency and dispersion, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IQR method\n",
    "Another robust method for labeling outliers is the IQR (interquartile range) method of outlier detection developed by John Tukey, the pioneer of exploratory data analysis. This was in the days of calculation and plotting by hand, so the datasets involved were typically small, and the emphasis was on understanding the story the data told. If you’ve seen a box-and-whisker plot (also a Tukey contribution), you’ve seen this method in action.1\n",
    "\n",
    "A box-and-whisker plot uses quartiles (points that divide the data into four groups of equal size) to plot the shape of the data. The box represents the 1st and 3rd quartiles, which are equal to the 25th and 75th percentiles. The line inside the box represents the 2nd quartile, which is the median.\n",
    "\n",
    "The interquartile range, which gives this method of outlier detection its name, is the range between the first and the third quartiles (the edges of the box). Tukey considered any data point that fell outside of either 1.5 times the IQR below the first – or 1.5 times the IQR above the third – quartile to be “outside” or “far out”. In a classic box-and-whisker plot, the ‘whiskers’ extend up to the last data point that is not “outside”.\n",
    "\n",
    "Box-and-whisker plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://colingorrie.github.io/images/galton-boxplot.png![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take the box-and-whisker plot above as an example. From the box, we can see that the median of the dataset falls at 66.5 inches, and that the first and third quartiles fall at approximately 64 and 70 inches, respectively. The whiskers show us that there are no outliers (as calculated by the IQR method) on the low end, but there is one on the high end, which is defined as over 78.25 inches.\n",
    "\n",
    "To automate the process of finding outliers by the IQR method, you can use the following Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def outliers_iqr(ys):\n",
    "    quartile_1, quartile_3 = np.percentile(ys, [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    return np.where((ys > upper_bound) | (ys < lower_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of using the interquartile range method is that, like the modified Z-score method, it uses a robust measure of dispersion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion\n",
    "So, which method should you use? First, there is little harm in using all of them and seeing what picture emerges: in exploratory data analysis, there is no inference being made. There is therefore no increased risk of Type 1 error (false positives). That said, in my opinion, the Z-score method has the least to offer, because it is dependent on non-robust measures, and fails to report any outliers with low-size datasets.\n",
    "\n",
    "Here are the results of running each of these functions on the Galton height data:\n",
    "\n",
    "Z-score: 56” (below); 78”, 79” (above)\n",
    "Modified Z-score: none\n",
    "IQR: 79” (above)\n",
    "As you can see, no method is returning radically different results from any other. In a dataset with 898 observations, the difference between 0 and 3 outliers is not great.\n",
    "\n",
    "One caveat: all of these methods will encounter problems with a strongly skewed dataset. If the data is distributed in a strongly asymmetrical fashion, it will need to be re-expressed before applying any of these methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "It is important to reiterate that these methods should not be used mechanically. They should be used to explore the data – they let you know which points might be worth a closer look. What to do with this information depends heavily on the situation. Sometimes it is appropriate to exclude outliers from a dataset to make a model trained on that dataset more predictive. Sometimes, however, the presence of outliers is a warning sign that the real-world process generating the data is more complicated than expected.\n",
    "\n",
    "As an astute commenter on CrossValidated put it: “Sometimes outliers are bad data, and should be excluded, such as typos. Sometimes they are Wayne Gretzky or Michael Jordan, and should be kept.” Domain knowledge and practical wisdom are the only ways to tell the difference.\n",
    "\n",
    "If you couldn’t tell, I’m a huge fan of Tukey’s work. See: Tukey, John (1977). Exploratory Data Analysis. Addison-Wesley, Reading MA. ↩\n",
    "\n",
    "Iglewicz, Boris and David Hoaglin (1993), How to Detect and Handle Outliers. American Society for Quality Control, Milwaukee WI. ↩\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to remove outlier\n",
    "#------------------------------------------------------------------------------\n",
    "# accept a dataframe, remove outliers, return cleaned data in a new dataframe\n",
    "# see http://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm\n",
    "#------------------------------------------------------------------------------\n",
    "def remove_outlier(df_in, col_name):\n",
    "    q1 = df_in[col_name].quantile(0.25)\n",
    "    q3 = df_in[col_name].quantile(0.75)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n",
    "    return df_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
